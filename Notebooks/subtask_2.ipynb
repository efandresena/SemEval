{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/efandresena/SemEval/blob/main/subtask_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djndEM6R0kXC",
        "outputId": "271117d3-53fc-4b54-c85f-1d0b4f619bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multililingual for subtask 2"
      ],
      "metadata": {
        "id": "CASHbqBMbKD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments, DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "JiRJpyQQgHAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workdir = \"/content/drive/MyDrive/NLP/SemEval\"\n",
        "\n",
        "CONFIG = {\n",
        "    'model': 'xlm-roberta-large',\n",
        "    'max_len': 250,\n",
        "    'epochs': 6,\n",
        "    'lr': 2e-5,\n",
        "    'batch': 16,\n",
        "    'grad_accum': 2,\n",
        "    'warmup': 0.1,\n",
        "    'weight_decay': 0.01,\n",
        "    'augment_factor': 3,\n",
        "}\n",
        "\n",
        "LABELS = ['gender/sexual', 'political', 'religious', 'racial/ethnic', 'other']\n",
        "\n",
        "SYNONYMS = {\n",
        "    'eng': {\n",
        "        'gender_sexual': {\n",
        "            'woman': ['female', 'lady', 'girl', 'bitch', 'slut', 'whore', 'cunt', 'hoe', 'skank', 'thot'],\n",
        "            'man': ['male', 'guy', 'dude', 'faggot', 'fag', 'pussy', 'beta', 'simp'],\n",
        "            'gay': ['LGBT', 'queer', 'homo', 'fag', 'dyke', 'tranny', 'trans', 'sissy', 'queen'],\n",
        "            'rape': ['assault', 'violence', 'molest', 'grope', 'force', 'violated'],\n",
        "            'marriage': ['union', 'wedlock', 'family', 'traditional marriage'],\n",
        "            # Additional common terms\n",
        "            'lesbian': ['dyke', 'lezzie'],\n",
        "            'transgender': ['tranny', 'shemale', 'he-she'],\n",
        "            'feminist': ['feminazi', 'SJW'],\n",
        "            'misogyny': ['hate women', 'women are inferior'],\n",
        "            'sexism': ['sexist', 'misogynist']\n",
        "        },\n",
        "        'political': {\n",
        "            'government': ['regime', 'authorities', 'dictators', 'tyrants', 'deep state'],\n",
        "            'election': ['vote', 'ballot', 'rigged', 'stolen election'],\n",
        "            'corruption': ['bribery', 'fraud', 'embezzlement', 'thieves', 'looters'],\n",
        "            'protest': ['rally', 'demonstration', 'riot', 'anarchy'],\n",
        "            # Additional common terms\n",
        "            'liberal': ['leftist', 'snowflake', 'woke', 'commie', 'socialist'],\n",
        "            'conservative': ['right-wing', 'fascist', 'nazi', 'bigot', 'MAGA'],\n",
        "            'traitor': ['sellout', 'enemy within', 'deep state puppet'],\n",
        "            'patriot': ['true patriot', 'real American'],\n",
        "            'democracy': ['fake democracy', 'stolen vote'],\n",
        "            'opposition': ['enemies', 'traitors', 'coup plotters']\n",
        "        },\n",
        "        'religious': {\n",
        "            'muslim': ['islamic', 'muzz', 'raghead', 'terrorist', 'jihadist'],\n",
        "            'christian': ['believer', 'infidel', 'kafir', 'crusader'],\n",
        "            'church': ['temple', 'mosque', 'synagogue', 'shrine'],\n",
        "            'god': ['deity', 'creator', 'allah', 'jesus'],\n",
        "            # Additional common terms\n",
        "            'islam': ['terrorist religion', 'sharia', 'radical islam'],\n",
        "            'christianity': ['fake religion', 'crusaders'],\n",
        "            'jew': ['kike', 'zionist', 'jewed'],\n",
        "            'hindu': ['idol worshipper'],\n",
        "            'atheist': ['godless', 'infidel'],\n",
        "            'blasphemy': ['insult god', 'offend religion']\n",
        "        },\n",
        "        'racial_ethnic': {\n",
        "            'black': ['african', 'nigger', 'nigga', 'monkey', 'ape', 'coon'],\n",
        "            'white': ['caucasian', 'cracker', 'white devil', 'colonizer'],\n",
        "            'racism': ['prejudice', 'bigotry', 'supremacy', 'hate'],\n",
        "            'immigrant': ['migrant', 'foreigner', 'illegal', 'invader', 'alien'],\n",
        "            # Additional common terms\n",
        "            'asian': ['chink', 'gook', 'slant-eye'],\n",
        "            'latino': ['beaner', 'wetback'],\n",
        "            'arab': ['sand nigger', 'camel jockey'],\n",
        "            'native': ['savage', 'redskin'],\n",
        "            'ethnic': ['tribal', 'primitive'],\n",
        "            'xenophobia': ['hate foreigners', 'go back home']\n",
        "        },\n",
        "        'other': {\n",
        "            'violence': ['brutality', 'aggression', 'kill', 'murder', 'slaughter'],\n",
        "            'hate': ['hostility', 'chuki', 'enmity', 'loathing'],\n",
        "            'war': ['conflict', 'battle', 'genocide', 'ethnic cleansing'],\n",
        "            # Additional terms\n",
        "            'dehumanize': ['animal', 'cockroach', 'weed', 'vermin', 'subhuman'],\n",
        "            'threat': ['kill', 'die', 'eliminate', 'wipe out'],\n",
        "            'incite': ['rally', 'mobilize', 'attack', 'burn'],\n",
        "            'disability': ['retard', 'cripple', 'spastic'],\n",
        "            'age': ['boomer', 'snowflake', 'old fart']\n",
        "        }\n",
        "    },\n",
        "    'swa': {\n",
        "        'gender_sexual': {\n",
        "            'mwanamke': ['mama', 'bibi', 'msichana', 'malaya', 'kahaba', 'dada', 'mrembo'],\n",
        "            'mwanamume': ['baba', 'bwana', 'mzee', 'mwanaume', 'shoga', 'shoga', 'ngombe'],\n",
        "            'ndoa': ['kuoana', 'harusi', 'ndoa ya jadi'],\n",
        "            # Additional common terms\n",
        "            'mwanamke': ['mwanamke wa mitaani', 'kiboko', 'dada wa mtaa'],\n",
        "            'shoga': ['msenge', 'shoga', 'mashoga', 'homosexual'],\n",
        "            'ngono': ['ngono', 'kufanya mapenzi', 'kudhalilisha'],\n",
        "            'feminism': ['feminazi', 'wanawake wenye hasira'],\n",
        "            'ubaguzi wa jinsia': ['sexism', 'misogyny']\n",
        "        },\n",
        "        'political': {\n",
        "            'serikali': ['hukuma', 'dola', 'viongozi', 'wadhalilishaji'],\n",
        "            'rais': ['mkuu', 'kiongozi', 'dikteta', 'mwizi'],\n",
        "            'uchaguzi': ['kura', 'uchaguzi ulioibiwa', 'rigged'],\n",
        "            'rushwa': ['ufisadi', 'kutoa rushwa', 'kuiba'],\n",
        "            # Additional common terms\n",
        "            'upinzani': ['maadui', 'wasaliti', 'wauzaji wa nchi'],\n",
        "            'wazalendo': ['watajua', 'wale wengine'],\n",
        "            'watajua': ['watajua hawajui', 'watajua'],\n",
        "            'chunga kura': ['secure vote', 'protect vote'],\n",
        "            'madoadoa': ['dots', 'madoadoa'],\n",
        "            'mende': ['cockroaches'],\n",
        "            'wabara': ['wabara waende kwao']\n",
        "        },\n",
        "        'religious': {\n",
        "            'mwislamu': ['muislamu', 'musalama', 'mgaidi', 'kaffir'],\n",
        "            'mkristo': ['muumini', 'mchawi', 'kafir'],\n",
        "            'dini': ['imani', 'dini ya kishenzi', 'dini ya kuficha'],\n",
        "            'mungu': ['mwenyezi mungu', 'allah'],\n",
        "            'islam': ['dini ya magaidi', 'sharia'],\n",
        "            'kristo': ['dini ya kishenzi', 'kristo'],\n",
        "            'kafir': ['kafir', 'kaffir'],\n",
        "            'chinja kafir': ['chinja kafir', 'kill infidel']\n",
        "        },\n",
        "        'racial_ethnic': {\n",
        "            'mweusi': ['mwafrika', 'mweusi', 'madoadoa'],\n",
        "            'mzungu': ['mweupe', 'mzungu', 'mkoloni'],\n",
        "            'kabila': ['jamii', 'kabila', 'ukabila'],\n",
        "            # Additional common terms\n",
        "            'mende': ['mende', 'cockroaches'],\n",
        "            'kwekwe': ['kwekwe', 'weeds'],\n",
        "            'madoadoa': ['madoadoa', 'spots'],\n",
        "            'wabara': ['wabara waende kwao'],\n",
        "            'wakuja': ['wakuja'],\n",
        "            'watajua': ['watajua hawajui']\n",
        "        },\n",
        "        'other': {\n",
        "            'vita': ['mapambano', 'mzozo', 'vita', 'chinja'],\n",
        "            'chuki': ['uadui', 'chuki', 'hasira'],\n",
        "            'ua': ['mauaji', 'kill', 'slaughter'],\n",
        "            # Additional common terms\n",
        "            'chinja': ['chinja', 'butcher'],\n",
        "            'mende': ['mende', 'cockroaches'],\n",
        "            'kwekwe': ['kwekwe', 'weeds'],\n",
        "            'madoadoa': ['madoadoa'],\n",
        "            'operation linda kura': ['protect vote'],\n",
        "            'hatupangwingwi': [\"we won't be told what to do\"]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "ywBBBerWGg5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import random\n",
        "\n",
        "def augment_text(text, category, lang, prob=0.5):\n",
        "    \"\"\"\n",
        "    Augment text by replacing words from the given category with synonyms.\n",
        "    Returns (new_text, was_changed)\n",
        "    \"\"\"\n",
        "    cat_syns = SYNONYMS.get(lang, {}).get(category, {})\n",
        "    if not cat_syns:\n",
        "        return text, False\n",
        "\n",
        "    words = text.split()\n",
        "    aug_words = []\n",
        "    changed = False\n",
        "\n",
        "    for word in words:\n",
        "        # Remove all punctuation for matching, but remember original form\n",
        "        cleaned = word.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "\n",
        "        if cleaned in cat_syns and random.random() < prob:\n",
        "            syn = random.choice(cat_syns[cleaned])\n",
        "\n",
        "            # Preserve capitalization\n",
        "            if word and word[0].isupper():\n",
        "                syn = syn.capitalize()\n",
        "\n",
        "            # Re-attach trailing punctuation if original had it\n",
        "            if word and word[-1] in string.punctuation:\n",
        "                # Find the last punctuation and attach it\n",
        "                trailing_punct = word[-1]\n",
        "                if len(word) > 1 and word[-2] in string.punctuation:\n",
        "                    trailing_punct = word[-2] + word[-1]  # handle !!, ??, etc.\n",
        "                syn += trailing_punct\n",
        "\n",
        "            aug_words.append(syn)\n",
        "            changed = True\n",
        "        else:\n",
        "            aug_words.append(word)\n",
        "\n",
        "    new_text = ' '.join(aug_words)\n",
        "    return new_text, changed\n",
        "\n",
        "\n",
        "def augment_df(df, lang, factor=3):\n",
        "    print(f\"Augmenting {lang}: {len(df)} samples\")\n",
        "\n",
        "    aug_texts = []\n",
        "    aug_labels = []\n",
        "    total_attempts = 0\n",
        "    successful_changes = 0\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        text = row['text']\n",
        "        label_vals = {label: row[label] for label in LABELS}\n",
        "        pos_labels = [l for l, v in label_vals.items() if v == 1]\n",
        "\n",
        "        if not pos_labels:\n",
        "            continue  # Skip non-toxic rows\n",
        "\n",
        "        # Adaptive number of augmentations per row\n",
        "        min_count = min(df[l].sum() for l in pos_labels)\n",
        "        if min_count < 50:\n",
        "            n_aug = factor * 4      # Very rare → aggressive augmentation\n",
        "        elif min_count < 150:\n",
        "            n_aug = factor * 3\n",
        "        elif min_count < 300:\n",
        "            n_aug = factor * 2\n",
        "        else:\n",
        "            n_aug = factor           # Common enough → moderate\n",
        "\n",
        "        for _ in range(n_aug):\n",
        "            total_attempts += 1\n",
        "            cat = random.choice(pos_labels)  # Pick one positive label to target\n",
        "            new_text, was_changed = augment_text(text, cat, lang, prob=0.9)\n",
        "\n",
        "            if was_changed and new_text != text:\n",
        "                aug_texts.append(new_text)\n",
        "                aug_labels.append(label_vals.copy())\n",
        "                successful_changes += 1\n",
        "\n",
        "    print(f\"  Total attempts: {total_attempts}\")\n",
        "    print(f\"  Successful augmentations: {successful_changes}\")\n",
        "\n",
        "    if aug_texts:\n",
        "        aug_df = pd.DataFrame({\n",
        "            'text': aug_texts,\n",
        "            **{label: [d[label] for d in aug_labels] for label in LABELS}\n",
        "        })\n",
        "        result = pd.concat([df, aug_df], ignore_index=True)\n",
        "        print(f\"  +{len(aug_df)} augmented → {len(result)} total samples\")\n",
        "        return result\n",
        "    else:\n",
        "        print(\"  No successful augmentations generated.\")\n",
        "        return df"
      ],
      "metadata": {
        "id": "XNsmU_V-Gmbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FocalLoss(nn.Module):\n",
        "  def __init__(self, alpha=0.25, gamma=2.0, pos_weight=None):\n",
        "      super().__init__()\n",
        "      self.alpha = alpha\n",
        "      self.gamma = gamma\n",
        "      self.pos_weight = pos_weight\n",
        "\n",
        "  def forward(self, inputs, targets):\n",
        "      bce = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none', pos_weight=self.pos_weight)\n",
        "      probs = torch.sigmoid(inputs)\n",
        "      pt = torch.where(targets == 1, probs, 1 - probs)\n",
        "      focal = (1 - pt) ** self.gamma\n",
        "      if self.alpha is not None:\n",
        "          alpha_w = torch.where(targets == 1, self.alpha, 1 - self.alpha)\n",
        "          focal = alpha_w * focal\n",
        "      return (focal * bce).mean()\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, *args, class_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        pos_weight = self.class_weights.to(outputs.logits.device) if self.class_weights is not None else None\n",
        "        loss = FocalLoss(alpha=0.25, gamma=2.0, pos_weight=pos_weight)(outputs.logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "class PolarizationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer(self.texts[idx], truncation=True, max_length=self.max_len, padding=False, return_tensors='pt')\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    probs = torch.sigmoid(torch.from_numpy(pred.predictions)).numpy()\n",
        "    preds = (probs > 0.5).astype(int)\n",
        "    f1_macro = f1_score(pred.label_ids, preds, average='macro', zero_division=0)\n",
        "    f1_per = f1_score(pred.label_ids, preds, average=None, zero_division=0)\n",
        "    metrics = {'f1_macro': f1_macro}\n",
        "    for i, label in enumerate(LABELS):\n",
        "        metrics[f'f1_{label.replace(\"/\", \"_\").replace(\" \", \"_\")}'] = f1_per[i]\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "Kv3cZOi0B9P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_combined(class_weight=True):\n",
        "  print(\"=\"*60)\n",
        "  print(\"COMBINED MULTILINGUAL TRAINING\")\n",
        "  print(\"=\"*60)\n",
        "\n",
        "  # Load data\n",
        "  eng_train = pd.read_csv(os.path.join(workdir, \"dev_phase/subtask2/train/eng.csv\"))\n",
        "  swa_train = pd.read_csv(os.path.join(workdir, \"dev_phase/subtask2/train/swa.csv\"))\n",
        "  eng_dev = pd.read_csv(os.path.join(workdir, \"dev_phase/subtask2/dev/eng.csv\"))\n",
        "  swa_dev = pd.read_csv(os.path.join(workdir, \"dev_phase/subtask2/dev/swa.csv\"))\n",
        "\n",
        "  # Augment\n",
        "  eng_train = augment_df(eng_train, 'eng', CONFIG['augment_factor'])\n",
        "  swa_train = augment_df(swa_train, 'swa', CONFIG['augment_factor'])\n",
        "\n",
        "  # Combine\n",
        "  combined_train = pd.concat([eng_train, swa_train], ignore_index=True)\n",
        "  print(f\"\\nCombined training: {len(combined_train)} samples\")\n",
        "  for label in LABELS:\n",
        "      print(f\"  {label}: {combined_train[label].sum()}\")\n",
        "\n",
        "  # Split validation\n",
        "  train_df, val_df = train_test_split(combined_train, test_size=0.15, random_state=42)\n",
        "\n",
        "  # Class weights\n",
        "  pos_counts = train_df[LABELS].sum()\n",
        "  neg_counts = len(train_df) - pos_counts\n",
        "  class_weights = torch.clamp(torch.tensor(neg_counts / (pos_counts + 1e-6), dtype=torch.float32), 1.0, 10.0)\n",
        "\n",
        "  # Model\n",
        "  tokenizer = AutoTokenizer.from_pretrained(CONFIG['model'])\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(CONFIG['model'], num_labels=5, problem_type=\"multi_label_classification\")\n",
        "\n",
        "  # Datasets\n",
        "  train_dataset = PolarizationDataset(train_df['text'].tolist(), train_df[LABELS].values.tolist(), tokenizer, CONFIG['max_len'])\n",
        "  val_dataset = PolarizationDataset(val_df['text'].tolist(), val_df[LABELS].values.tolist(), tokenizer, CONFIG['max_len'])\n",
        "\n",
        "  # Training\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=\"./results_combined\",\n",
        "      num_train_epochs=CONFIG['epochs'],\n",
        "      learning_rate=CONFIG['lr'],\n",
        "      per_device_train_batch_size=CONFIG['batch'],\n",
        "      per_device_eval_batch_size=CONFIG['batch'] * 2,\n",
        "      gradient_accumulation_steps=CONFIG['grad_accum'],\n",
        "      warmup_ratio=CONFIG['warmup'],\n",
        "      weight_decay=CONFIG['weight_decay'],\n",
        "      eval_strategy=\"steps\",\n",
        "      eval_steps=100,\n",
        "      save_strategy=\"steps\",\n",
        "      save_steps=100,\n",
        "      load_best_model_at_end=True,\n",
        "      metric_for_best_model=\"f1_macro\",\n",
        "      greater_is_better=True,\n",
        "      logging_steps=50,\n",
        "      fp16=True,\n",
        "      report_to=\"none\",\n",
        "      save_total_limit=2,\n",
        "  )\n",
        "  if class_weight:\n",
        "    trainer = WeightedTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
        "        class_weights=class_weights\n",
        "    )\n",
        "  else:\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
        "    )\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "    # Evaluate\n",
        "  final = trainer.evaluate()\n",
        "  print(f\"\\nFinal F1 Macro: {final['eval_f1_macro']:.4f}\")\n",
        "\n",
        "  # Find threshold\n",
        "  val_preds = trainer.predict(val_dataset)\n",
        "  val_probs = torch.sigmoid(torch.tensor(val_preds.predictions)).numpy()\n",
        "  best_thresh, best_f1 = 0.5, 0\n",
        "  for thresh in np.arange(0.3, 0.40, 0.55):\n",
        "      preds = (val_probs > thresh).astype(int)\n",
        "      f1 = f1_score(val_df[LABELS].values, preds, average='macro', zero_division=0)\n",
        "      if f1 > best_f1:\n",
        "          best_f1, best_thresh = f1, thresh\n",
        "  print(f\"Best threshold: {best_thresh:.2f} (F1: {best_f1:.4f})\")\n",
        "\n",
        "  # Predict English\n",
        "  eng_dataset = PolarizationDataset(eng_dev['text'].tolist(), [[0]*5]*len(eng_dev), tokenizer, CONFIG['max_len'])\n",
        "  eng_preds = trainer.predict(eng_dataset)\n",
        "  eng_probs = torch.sigmoid(torch.tensor(eng_preds.predictions)).numpy()\n",
        "  eng_binary = (eng_probs > best_thresh).astype(int)\n",
        "  eng_result = pd.DataFrame(eng_binary, columns=LABELS)\n",
        "  eng_result.insert(0, 'id', eng_dev['id'])\n",
        "  eng_result.to_csv(os.path.join(workdir, \"pred_eng_mul.csv\"), index=False)\n",
        "  print(f\"\\n✓ English predictions saved\")\n",
        "  print(f\"Distribution:\\n{eng_result[LABELS].sum()}\")\n",
        "\n",
        "  # Predict Swahili\n",
        "  swa_dataset = PolarizationDataset(swa_dev['text'].tolist(), [[0]*5]*len(swa_dev), tokenizer, CONFIG['max_len'])\n",
        "  swa_preds = trainer.predict(swa_dataset)\n",
        "  swa_probs = torch.sigmoid(torch.tensor(swa_preds.predictions)).numpy()\n",
        "  swa_binary = (swa_probs > best_thresh).astype(int)\n",
        "  swa_result = pd.DataFrame(swa_binary, columns=LABELS)\n",
        "  swa_result.insert(0, 'id', swa_dev['id'])\n",
        "  swa_result.to_csv(os.path.join(workdir, \"pred_swa_mul.csv\"), index=False)\n",
        "  print(f\"\\n✓ Swahili predictions saved\")\n",
        "  print(f\"Distribution:\\n{swa_result[LABELS].sum()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "H1eGcxFVgE-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_combined(class_weight=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U-HFSoTPCVYZ",
        "outputId": "e0b8b8f8-a80e-4e31-b9fd-d94a47b5e735"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "COMBINED MULTILINGUAL TRAINING\n",
            "============================================================\n",
            "Augmenting eng: 3222 samples\n",
            "  Total attempts: 5703\n",
            "  Successful augmentations: 504\n",
            "  +504 augmented → 3726 total samples\n",
            "Augmenting swa: 6991 samples\n",
            "  Total attempts: 12267\n",
            "  Successful augmentations: 131\n",
            "  +131 augmented → 7122 total samples\n",
            "\n",
            "Combined training: 10848 samples\n",
            "  gender/sexual: 244\n",
            "  political: 1888\n",
            "  religious: 504\n",
            "  racial/ethnic: 2807\n",
            "  other: 754\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1101' max='1734' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1101/1734 26:02 < 14:59, 0.70 it/s, Epoch 3.81/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Gender Sexual</th>\n",
              "      <th>F1 Political</th>\n",
              "      <th>F1 Religious</th>\n",
              "      <th>F1 Racial Ethnic</th>\n",
              "      <th>F1 Other</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.306500</td>\n",
              "      <td>0.277445</td>\n",
              "      <td>0.162139</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.530120</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.280576</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.270800</td>\n",
              "      <td>0.232237</td>\n",
              "      <td>0.227488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.728682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.408759</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.222500</td>\n",
              "      <td>0.229803</td>\n",
              "      <td>0.423706</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.732558</td>\n",
              "      <td>0.688297</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.199200</td>\n",
              "      <td>0.188028</td>\n",
              "      <td>0.457698</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.757315</td>\n",
              "      <td>0.785185</td>\n",
              "      <td>0.745989</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.205900</td>\n",
              "      <td>0.186335</td>\n",
              "      <td>0.448233</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.740443</td>\n",
              "      <td>0.755245</td>\n",
              "      <td>0.745476</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.183700</td>\n",
              "      <td>0.182779</td>\n",
              "      <td>0.457013</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.791367</td>\n",
              "      <td>0.730539</td>\n",
              "      <td>0.763158</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.173800</td>\n",
              "      <td>0.176243</td>\n",
              "      <td>0.467086</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.782435</td>\n",
              "      <td>0.785185</td>\n",
              "      <td>0.767810</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.156100</td>\n",
              "      <td>0.180064</td>\n",
              "      <td>0.475823</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>0.787546</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.747126</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.152600</td>\n",
              "      <td>0.174309</td>\n",
              "      <td>0.508443</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>0.795009</td>\n",
              "      <td>0.771242</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.164179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.136200</td>\n",
              "      <td>0.176425</td>\n",
              "      <td>0.497663</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.782931</td>\n",
              "      <td>0.794118</td>\n",
              "      <td>0.759259</td>\n",
              "      <td>0.108527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.139400</td>\n",
              "      <td>0.172475</td>\n",
              "      <td>0.521481</td>\n",
              "      <td>0.075472</td>\n",
              "      <td>0.797020</td>\n",
              "      <td>0.789116</td>\n",
              "      <td>0.771887</td>\n",
              "      <td>0.173913</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1734' max='1734' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1734/1734 45:44, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Gender Sexual</th>\n",
              "      <th>F1 Political</th>\n",
              "      <th>F1 Religious</th>\n",
              "      <th>F1 Racial Ethnic</th>\n",
              "      <th>F1 Other</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.306500</td>\n",
              "      <td>0.277445</td>\n",
              "      <td>0.162139</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.530120</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.280576</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.270800</td>\n",
              "      <td>0.232237</td>\n",
              "      <td>0.227488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.728682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.408759</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.222500</td>\n",
              "      <td>0.229803</td>\n",
              "      <td>0.423706</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.732558</td>\n",
              "      <td>0.688297</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.199200</td>\n",
              "      <td>0.188028</td>\n",
              "      <td>0.457698</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.757315</td>\n",
              "      <td>0.785185</td>\n",
              "      <td>0.745989</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.205900</td>\n",
              "      <td>0.186335</td>\n",
              "      <td>0.448233</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.740443</td>\n",
              "      <td>0.755245</td>\n",
              "      <td>0.745476</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.183700</td>\n",
              "      <td>0.182779</td>\n",
              "      <td>0.457013</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.791367</td>\n",
              "      <td>0.730539</td>\n",
              "      <td>0.763158</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.173800</td>\n",
              "      <td>0.176243</td>\n",
              "      <td>0.467086</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.782435</td>\n",
              "      <td>0.785185</td>\n",
              "      <td>0.767810</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.156100</td>\n",
              "      <td>0.180064</td>\n",
              "      <td>0.475823</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>0.787546</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.747126</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.152600</td>\n",
              "      <td>0.174309</td>\n",
              "      <td>0.508443</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>0.795009</td>\n",
              "      <td>0.771242</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.164179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.136200</td>\n",
              "      <td>0.176425</td>\n",
              "      <td>0.497663</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.782931</td>\n",
              "      <td>0.794118</td>\n",
              "      <td>0.759259</td>\n",
              "      <td>0.108527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.139400</td>\n",
              "      <td>0.172475</td>\n",
              "      <td>0.521481</td>\n",
              "      <td>0.075472</td>\n",
              "      <td>0.797020</td>\n",
              "      <td>0.789116</td>\n",
              "      <td>0.771887</td>\n",
              "      <td>0.173913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.120300</td>\n",
              "      <td>0.175915</td>\n",
              "      <td>0.522939</td>\n",
              "      <td>0.075472</td>\n",
              "      <td>0.792381</td>\n",
              "      <td>0.773333</td>\n",
              "      <td>0.765178</td>\n",
              "      <td>0.208333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.126400</td>\n",
              "      <td>0.171300</td>\n",
              "      <td>0.549002</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.794918</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.790361</td>\n",
              "      <td>0.291139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.112700</td>\n",
              "      <td>0.177284</td>\n",
              "      <td>0.547991</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.793478</td>\n",
              "      <td>0.786667</td>\n",
              "      <td>0.768700</td>\n",
              "      <td>0.280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.102000</td>\n",
              "      <td>0.182813</td>\n",
              "      <td>0.556597</td>\n",
              "      <td>0.148148</td>\n",
              "      <td>0.800725</td>\n",
              "      <td>0.797297</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.272109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.103300</td>\n",
              "      <td>0.181200</td>\n",
              "      <td>0.562086</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.794224</td>\n",
              "      <td>0.807947</td>\n",
              "      <td>0.767630</td>\n",
              "      <td>0.322981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.104700</td>\n",
              "      <td>0.178553</td>\n",
              "      <td>0.554101</td>\n",
              "      <td>0.081633</td>\n",
              "      <td>0.805915</td>\n",
              "      <td>0.816327</td>\n",
              "      <td>0.758939</td>\n",
              "      <td>0.307692</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final F1 Macro: 0.5621\n",
            "Best threshold: 0.30 (F1: 0.5801)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ English predictions saved\n",
            "Distribution:\n",
            "gender/sexual     0\n",
            "political        56\n",
            "religious         6\n",
            "racial/ethnic    19\n",
            "other             1\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Swahili predictions saved\n",
            "Distribution:\n",
            "gender/sexual      2\n",
            "political          9\n",
            "religious         14\n",
            "racial/ethnic    159\n",
            "other             23\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qPY3UHZTRk5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}