{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST6hxfSM5LSF"
      },
      "source": [
        "# SUBTASK 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1bkvDjq5DKk",
        "outputId": "96ac3784-5dfd-4077-b7b3-3696427231b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/CSCE 689')\n",
        "from BERT import BERT\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import Markdown as md\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "hBf06I5w486b"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87e_8k4m5RzC"
      },
      "source": [
        "## Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/CSCE 689/HW3/twitter_sentiment_train.csv')\n",
        "dev_df = pd.read_csv('/content/drive/MyDrive/CSCE 689/HW3/twitter_sentiment_dev.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/CSCE 689/HW3/twitter_sentiment_test.csv')"
      ],
      "metadata": {
        "id": "j64d2d415d_3"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()\n",
        "dev_df.head()\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "8M4hN_2p5gI2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "4c980324-5d8f-4424-c119-9ce7c3b28b24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  sentiment                                               text\n",
              "0  3870   negative         @user the first two seasons are just so\n",
              "1  3975   negative  @user The first two series were great... now i..."
            ],
            "text/html": [
              "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3870</td>\n      <td>negative</td>\n      <td>@user the first two seasons are just so</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3975</td>\n      <td>negative</td>\n      <td>@user The first two series were great... now i...</td>\n    </tr>\n    <tr>\n      <th>2</th>
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.sentiment.value_counts()\n",
        "dev_df.sentiment.value_counts()\n",
        "test_df.sentiment.value_counts()"
      ],
      "metadata": {
        "id": "P3P-iJ0o5gO_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "91752b0f-7b79-430c-a636-96c4349195b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "negative    1000\n",
              "positive     999\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO2Wj2y85hC4"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "Qn6GjM2q5hF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3587b640-c782-4660-c3d5-e21e1edc223a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(data, tokenizer):\n",
        "    token_ids = []\n",
        "    attention_masks = []\n",
        "    \n",
        "    for text in tqdm(data['text']):\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens = True,\n",
        "            max_length = 64,\n",
        "            padding = 'max_length',\n",
        "            truncation = True,\n",
        "            return_attention_mask = True,\n",
        "            return_tensors = 'pt'\n",
        "        )\n",
        "        token_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "        \n",
        "    token_ids = torch.cat(token_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    \n",
        "    return token_ids, attention_masks\n",
        "\n",
        "train_token_ids, train_attention_masks = tokenize_text(train_df, tokenizer)\n",
        "dev_token_ids, dev_attention_masks = tokenize_text(dev_df, tokenizer)\n",
        "test_token_ids, test_attention_masks = tokenize_text(test_df, tokenizer)"
      ],
      "metadata": {
        "id": "iW0y8LwU5hOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41315e98-c172-4628-97a7-17983b632924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0%|          | 0/1999 [00:00<?, ?it/s]\r2%|2         | 34/1999 [00:00<00:04, 439.02it/s]\r4%|3         | 69/1999 [00:00<00:03, 563.85it/s]\r5%|5         | 103/1999 [00:00<00:03, 532.78it/s]\r7%|7         | 137/1999 [00:00<00:03, 517.44it/s]\r8%|8         | 170/1999 [00:00<00:03, 529.57it/s]\r10%|#         | 205/1999 [00:00<00:03, 528.08it/s]\r12%|#1        | 240/1999 [00:00<00:03, 532.41it/s]\r13%|#3        | 274/1999 [00:00<00:03, 527.73it/s]\r15%|#4        | 308/1999 [00:00<00:03, 532.12it/s]\r16%|#6        | 343/1999 [00:00<00:03, 527.46it/s]\r18%|#8        | 378/1999 [00:00<00:03, 530.93it/s]\r20%|##        | 412/1999 [00:00<00:03, 526.43it/s]\r21%|##1       | 447/1999 [00:00<00:02, 530.13it/s]\r23%|##3       | 481/1999 [00:00<00:02, 532.35it/s]\r25%|##4       | 515/1999 [00:00<00:02, 526.43it/s]\r26%|##6       | 550/1999 [00:01<00:02, 531.06it/s]\r28%|##8       | 584/1999 [00:01<00:02, 526.04it/s]\r29%|##9       | 619/1999 [00:01<00:02, 529.98it/s]\r31%|###1      | 653/1999 [00:01<00:02, 531.84it/s]\r33%|###2      | 688/1999 [00:01<00:02, 526.96it/s]\r34%|###4      | 723/1999 [00:01<00:02, 530.73it/s]\r36%|###6      | 757/1999 [00:01<00:02, 525.86it/s]\r38%|###7      | 792/1999 [00:01<00:02, 529.74it/s]\r39%|###9      | 826/1999 [00:01<00:02, 531.63it/s]\r41%|####      | 861/1999 [00:01<00:02, 527.02it/s]\r43%|####2     | 896/1999 [00:01<00:02, 530.49it/s]\r44%|####4     | 930/1999 [00:01<00:02, 525.89it/s]\r46%|####5     | 965/1999 [00:01<00:01, 529.98it/s]\r48%|####7     | 999/1999 [00:01<00:01, 531.78it/s]\r49%|####9     | 1034/1999 [00:01<00:01, 527.12it/s]\r51%|#####1    | 1069/1999 [00:02<00:01, 530.29it/s]\r53%|#####2    | 1103/1999 [00:02<00:01, 532.05it/s]\r54%|#####4    | 1138/1999 [00:02<00:01, 527.01it/s]\r56%|#####6    | 1172/1999 [00:02<00:01, 530.95it/s]\r58%|#####7    | 1206/1999 [00:02<00:01, 532.96it/s]\r59%|#####9    | 1241/1999 [00:02<00:01, 528.02it/s]\r61%|######1   | 1276/1999 [00:02<00:01, 531.39it/s]\r63%|######2   | 1310/1999 [00:02<00:01, 526.42it/s]\r64%|######4   | 1345/1999 [00:02<00:01, 529.98it/s]\r66%|######6   | 1379/1999 [00:02<00:01, 531.73it/s]\r68%|######7   | 1414/1999 [00:02<00:01, 526.97it/s]\r69%|######9   | 1449/1999 [00:02<00:01, 530.54it/s]\r71%|#######1  | 1483/1999 [00:03<00:00, 532.22it/s]\r73%|#######2  | 1518/1999 [00:03<00:00, 527.56it/s]\r74%|#######4  | 1553/1999 [00:03<00:00, 530.40it/s]\r76%|#######6  | 1587/1999 [00:03<00:00, 532.17it/s]\r78%|#######7  | 1622/1999 [00:03<00:00, 527.52it/s]\r79%|#######9  | 1657/1999 [00:03<00:00, 530.34it/s]\r81%|########1 | 1691/1999 [00:03<00:00, 532.24it/s]\r83%|########2 | 1726/1999 [00:03<00:00, 527.56it/s]\r84%|########4 | 1761/1999 [00:03<00:00, 530.34it/s]\r86%|########6 | 1795/1999 [00:03<00:00, 532.19it/s]\r88%|########7 | 1830/1999 [00:03<00:00, 527.42it/s]\r89%|########9 | 1865/1999 [00:03<00:00, 530.08it/s]\r91%|######### | 1899/1999 [00:03<00:00, 531.96it/s]\r93%|#########2| 1934/1999 [00:03<00:00, 527.28it/s]\r94%|#########4| 1969/1999 [00:03<00:00, 530.12it/s]\r96%|#########6| 1999/1999 [00:03<00:00, 537.49it/s]\n",
            "0%|          | 0/200 [00:00<?, ?it/s]\r4%|4         | 8/200 [00:00<00:00, 100.22it/s]\r10%|#         | 20/200 [00:00<00:00, 150.39it/s]\r16%|#6        | 31/200 [00:00<00:00, 169.37it/s]\r22%|##2       | 44/200 [00:00<00:00, 189.69it/s]\r30%|###       | 60/200 [00:00<00:00, 212.19it/s]\r37%|###7      | 74/200 [00:00<00:00, 219.08it/s]\r45%|####5     | 90/200 [00:00<00:00, 230.15it/s]\r53%|#####2    | 105/200 [00:00<00:00, 236.43it/s]\r61%|######    | 121/200 [00:00<00:00, 241.69it/s]\r69%|######8   | 138/200 [00:00<00:00, 247.01it/s]\r77%|#######7  | 154/200 [00:00<00:00, 252.02it/s]\r85%|########4 | 170/200 [00:00<00:00, 255.85it/s]\r93%|#########2| 185/200 [00:00<00:00, 258.97it/s]\r100%|##########| 200/200 [00:00<00:00, 260.91it/s]\n",
            "0%|          | 0/200 [00:00<?, ?it/s]\r5%|5         | 10/200 [00:00<00:00, 137.64it/s]\r11%|#1        | 21/200 [00:00<00:00, 172.95it/s]\r18%|#7        | 35/200 [00:00<00:00, 216.51it/s]\r25%|##4       | 50/200 [00:00<00:00, 240.24it/s]\r32%|###2      | 65/200 [00:00<00:00, 253.91it/s]\r40%|####      | 80/200 [00:00<00:00, 262.11it/s]\r48%|####7     | 95/200 [00:00<00:00, 267.75it/s]\r55%|#####5    | 110/200 [00:00<00:00, 273.78it/s]\r63%|######3   | 126/200 [00:00<00:00, 279.79it/s]\r71%|#######1  | 142/200 [00:00<00:00, 285.34it/s]\r79%|#######9  | 158/200 [00:00<00:00, 289.43it/s]\r87%|########7 | 175/200 [00:00<00:00, 294.57it/s]\r95%|#########5| 191/200 [00:00<00:00, 298.81it/s]\r100%|##########| 200/200 [00:00<00:00, 298.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1xI52j15hUn"
      },
      "source": [
        "## Creating Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, token_ids, attention_masks, labels):\n",
        "        self.token_ids = token_ids\n",
        "        self.attention_masks = attention_masks\n",
        "        self.labels = labels\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.token_ids[idx], self.attention_masks[idx], self.labels[idx]\n",
        "    \n",
        "train_labels = torch.tensor(train_df['sentiment'].replace({'positive': 1, 'negative': 0}).values)\n",
        "dev_labels = torch.tensor(dev_df['sentiment'].replace({'positive': 1, 'negative': 0}).values)\n",
        "test_labels = torch.tensor(test_df['sentiment'].replace({'positive': 1, 'negative': 0}).values)\n",
        "\n",
        "train_dataset = TwitterDataset(train_token_ids, train_attention_masks, train_labels)\n",
        "dev_dataset = TwitterDataset(dev_token_ids, dev_attention_masks, dev_labels)\n",
        "test_dataset = TwitterDataset(test_token_ids, test_attention_masks, test_labels)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "J2vE4f5w5hYo"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJj-9pD35hby"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BERT().to(device)\n",
        "model"
      ],
      "metadata": {
        "id": "oX94gG9p5hdx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "34c1b96a-0495-46f4-a477-d64e432a243e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERT(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELU(approximate='none')\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (softmax): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG759-Vz5hga"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "loss_fn = torch.nn.NLLLoss()\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for token_ids, attention_masks, labels in tqdm(dataloader):\n",
        "        token_ids = token_ids.to(device)\n",
        "        attention_masks = attention_masks.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        outputs = model(token_ids, attention_masks)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "    avg_train_loss = total_loss / len(dataloader)\n",
        "    return avg_train_loss\n",
        "\n",
        "def evaluate(model, dataloader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for token_ids, attention_masks, labels in dataloader:\n",
        "            token_ids = token_ids.to(device)\n",
        "            attention_masks = attention_masks.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(token_ids, attention_masks)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            preds = torch.argmax(outputs, dim=1).flatten()\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            \n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, predictions, true_labels\n",
        "\n",
        "best_dev_acc = 0\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch+1}/{epochs}')\n",
        "    \n",
        "    train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, loss_fn, device)\n",
        "    print(f'  Train Loss: {train_loss:.4f}')\n",
        "    \n",
        "    dev_loss, dev_preds, dev_labels_true = evaluate(model, dev_dataloader, loss_fn, device)\n",
        "    dev_acc = accuracy_score(dev_labels_true, dev_preds)\n",
        "    print(f'  Dev Loss: {dev_loss:.4f}, Dev Accuracy: {dev_acc:.4f}')\n",
        "    \n",
        "    if dev_acc > best_dev_acc:\n",
        "        best_dev_acc = dev_acc\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/CSCE 689/HW3/best_bert_model.pt')\n",
        "        print('  Saved best model.')\n",
        "        \n",
        "    print('-'*30)\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/CSCE 689/HW3/best_bert_model.pt'))\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "g-3-N01Q5hlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32135c34-8c11-487b-b39b-e0186c3dd795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "100%|##########| 125/125 [00:23<00:00,  5.37it/s]\n",
            "  Train Loss: 0.6953\n",
            "  Dev Loss: 0.6974, Dev Accuracy: 0.5000\n",
            "  Saved best model.\n",
            "------------------------------\n",
            "Epoch 2/5\n",
            "100%|##########| 125/125 [00:23<00:00,  5.34it/s]\n",
            "  Train Loss: 0.6957\n",
            "  Dev Loss: 0.6972, Dev Accuracy: 0.5000\n",
            "------------------------------\n",
            "Epoch 3/5\n",
            "100%|##########| 125/125 [00:23<00:00,  5.37it/s]\n",
            "  Train Loss: 0.6959\n",
            "  Dev Loss: 0.6970, Dev Accuracy: 0.5000\n",
            "------------------------------\n",
            "Epoch 4/5\n",
            "100%|##########| 125/125 [00:23<00:00,  5.36it/s]\n",
            "  Train Loss: 0.6955\n",
            "  Dev Loss: 0.6968, Dev Accuracy: 0.5000\n",
            "------------------------------\n",
            "Epoch 5/5\n",
            "100%|##########| 125/125 [00:23<00:00,  5.36it/s]\n",
            "  Train Loss: 0.6955\n",
            "  Dev Loss: 0.6966, Dev Accuracy: 0.5000\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQhM4B_M5i4y"
      },
      "source": [
        "## Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_preds, test_labels_true = evaluate(model, test_dataloader, loss_fn, device)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy_score(test_labels_true, test_preds):.4f}')\n",
        "print(classification_report(test_labels_true, test_preds, target_names=['negative', 'positive']))"
      ],
      "metadata": {
        "id": "W7D6i9oB5i7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34997c45-13be-46b5-8c01-8b1e06d99723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.6975, Test Accuracy: 0.4900\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.49      1.00      0.66       100\n",
            "    positive       0.00      0.00      0.00       100\n",
            "\n",
            "    accuracy                           0.49       200\n",
            "   macro avg       0.25      0.50      0.33       200\n",
            "weighted avg       0.24      0.49      0.33       200\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
