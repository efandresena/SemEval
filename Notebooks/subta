{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/efandresena/SemEval/blob/main/subtask_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST6hxfSM5LSF"
      },
      "source": [
        "# SUBTASK 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1bkvDjq5DKk",
        "outputId": "96ac3784-5dfd-4077-b7b3-3696427231b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        "    pipeline,\n",
        "    AutoConfig\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "hWr2yPM-GTcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONFIGURATION ENGLISH"
      ],
      "metadata": {
        "id": "MgOU3Nw5uMwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wokdir = \"/content/drive/MyDrive/BLOC#4/NLP/SemEval\" # CHNAGE IT ACCORDING TO YOUR WRKING DIRECTORY\n",
        "\n",
        "CONFIG = {\n",
        "    'language': 'eng',\n",
        "    'data_subfolder': \"dev_phase/subtask1/\",\n",
        "    'model_name': 'cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
        "    'max_length': 300,\n",
        "    'num_epochs': 8,\n",
        "    'learning_rate': 2e-5,\n",
        "    'batch_size': 16,\n",
        "    'weight_decay': 0.01,\n",
        "    'warmup_ratio': 0.2,\n",
        "    'gradient_accumulation_steps': 4,\n",
        "    'val_split': 0.01,\n",
        "    'use_class_weights': True,\n",
        "}\n",
        "\n",
        "LANG = CONFIG['language']\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"BINARY CLASSIFICATION TRAINING: {LANG.upper()}\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oTbTaU8svvN",
        "outputId": "919e47f2-4a3e-4334-f591-2e5241fe4082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BINARY CLASSIFICATION TRAINING: ENG\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA LOADER"
      ],
      "metadata": {
        "id": "lHX0ucmKuKe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PolarizationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer(\n",
        "            str(self.texts[idx]),\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=False,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item['labels'] = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, class_weights=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if self.class_weights is not None:\n",
        "            loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "            loss = loss_fct(logits, labels)\n",
        "        else:\n",
        "            loss = outputs.loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    labels = p.label_ids\n",
        "    return {\n",
        "        'f1_macro': f1_score(labels, preds, average='macro'),\n",
        "        'f1_binary': f1_score(labels, preds, pos_label=1),\n",
        "        'f1_weighted': f1_score(labels, preds, average='weighted'),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "n97K3vjCsyZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOADING DATA"
      ],
      "metadata": {
        "id": "4b1h0ZgLuGZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"\\nLoading training data for {LANG}...\")\n",
        "train_path = os.path.join(wokdir, CONFIG['data_subfolder'], f\"train/{LANG}.csv\") # Changed workdir to wokdir\n",
        "df = pd.read_csv(train_path)\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"Class distribution:\\n{df['polarization'].value_counts()}\")\n",
        "print(f\"Class distribution %:\\n{df['polarization'].value_counts(normalize=True)*100}\")\n",
        "\n",
        "class_counts = df['polarization'].value_counts()\n",
        "imbalance_ratio = class_counts.max() / class_counts.min()\n",
        "print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "class_weights = None\n",
        "if CONFIG['use_class_weights']:\n",
        "    classes = df['polarization'].unique()\n",
        "    weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=classes,\n",
        "        y=df['polarization']\n",
        "    )\n",
        "    class_weights = torch.FloatTensor(weights)\n",
        "    if torch.cuda.is_available():\n",
        "        class_weights = class_weights.cuda()\n",
        "    print(f\"Class weights: {weights}\")\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=CONFIG['val_split'],\n",
        "    stratify=df['polarization'],\n",
        "    random_state=42\n",
        ")\n",
        "print(f\"\\nTrain: {len(train_df)} | Val: {len(val_df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv4EYQrns4Ru",
        "outputId": "b78c4439-46a0-457a-e4cb-e48b64ff6589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading training data for hau...\n",
            "Total samples: 3651\n",
            "Class distribution:\n",
            "polarization\n",
            "0    3259\n",
            "1     392\n",
            "Name: count, dtype: int64\n",
            "Class distribution %:\n",
            "polarization\n",
            "0    89.263216\n",
            "1    10.736784\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Class imbalance ratio: 8.31:1\n",
            "\n",
            "Train: 3614 | Val: 37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOADING MODEL"
      ],
      "metadata": {
        "id": "pJYvlu37uDa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"\\nLoading model: {CONFIG['model_name']}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "\n",
        "config = AutoConfig.from_pretrained(CONFIG['model_name'])\n",
        "config.num_labels = 2\n",
        "config.id2label = {0: \"not_polarized\", 1: \"polarized\"}\n",
        "config.label2id = {\"not_polarized\": 0, \"polarized\": 1}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    CONFIG['model_name'],\n",
        "    config=config,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "print(f\"Model output labels: {model.config.num_labels}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygFJ5JaDs7Fu",
        "outputId": "1197ffbf-df64-4b53-efec-b27c20402251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading model: cardiffnlp/twitter-roberta-base-sentiment-latest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output labels: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING PARAMETERS"
      ],
      "metadata": {
        "id": "_oc6WTZYt_n5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dataset = PolarizationDataset(\n",
        "    train_df['text'].tolist(),\n",
        "    train_df['polarization'].tolist(),\n",
        "    tokenizer,\n",
        "    CONFIG['max_length']\n",
        ")\n",
        "val_dataset = PolarizationDataset(\n",
        "    val_df['text'].tolist(),\n",
        "    val_df['polarization'].tolist(),\n",
        "    tokenizer,\n",
        "    CONFIG['max_length']\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./results_{LANG}\",\n",
        "    num_train_epochs=CONFIG['num_epochs'],\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    per_device_train_batch_size=CONFIG['batch_size'],\n",
        "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
        "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
        "    weight_decay=CONFIG['weight_decay'],\n",
        "    warmup_ratio=CONFIG['warmup_ratio'],\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    logging_steps=25,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer_class = WeightedTrainer if CONFIG['use_class_weights'] else Trainer\n"
      ],
      "metadata": {
        "id": "r-9eduNps-V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINER"
      ],
      "metadata": {
        "id": "BusgEGfDt9_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer = trainer_class(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
        "    class_weights=class_weights if CONFIG['use_class_weights'] else None\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL VALIDATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "final_eval = trainer.evaluate()\n",
        "print(final_eval)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "hE1K24kPtAj2",
        "outputId": "650f279e-ab14-4cbf-a4ea-480104e52be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STARTING TRAINING\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [400/400 05:04, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Binary</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.483700</td>\n",
              "      <td>0.470996</td>\n",
              "      <td>0.774634</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.789534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.424200</td>\n",
              "      <td>0.457423</td>\n",
              "      <td>0.813910</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.821599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.318100</td>\n",
              "      <td>0.382118</td>\n",
              "      <td>0.869048</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.878788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.216200</td>\n",
              "      <td>0.365144</td>\n",
              "      <td>0.825397</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.842713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.146800</td>\n",
              "      <td>0.413817</td>\n",
              "      <td>0.895238</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.905628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.112900</td>\n",
              "      <td>0.424760</td>\n",
              "      <td>0.863636</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.876033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.066200</td>\n",
              "      <td>0.435730</td>\n",
              "      <td>0.895238</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.905628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.065600</td>\n",
              "      <td>0.449683</td>\n",
              "      <td>0.895238</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.905628</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL VALIDATION RESULTS\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.41381749510765076, 'eval_f1_macro': 0.8952380952380952, 'eval_f1_binary': 0.8571428571428571, 'eval_f1_weighted': 0.9056277056277057, 'eval_runtime': 0.0808, 'eval_samples_per_second': 408.645, 'eval_steps_per_second': 37.15, 'epoch': 8.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVING MODEL TO HUGGING FACE"
      ],
      "metadata": {
        "id": "BC8VnaCgwwjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, Repository\n",
        "from huggingface_hub import upload_folder\n",
        "from huggingface_hub import login\n",
        "login()\n"
      ],
      "metadata": {
        "id": "9uteqfX6w0K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "HF_MODEL_NAME = \"mirindraf/aims-sentiment-analysis\"\n",
        "api = HfApi()\n",
        "api.create_repo(\n",
        "    repo_id=HF_MODEL_NAME,\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True\n",
        ")\n",
        "upload_folder(\n",
        "    folder_path=model_folder,\n",
        "    repo_id=HF_MODEL_NAME,\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "MPVwSIw_yaPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load from hugging face\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(HF_MODEL_NAME)\n",
        "# Create a new Trainer (no training needed, just for inference)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "102c3e8ad45545d8b612e7709bb79428",
            "aba9ca82ccf546b1b52c4f65dbe22b05",
            "ec1934b7851e4e4984015c4f98333a26",
            "b154731f2bea45f68eeabf0a8bc2946d",
            "c0fd4ffd48574c34b237a8959ac8b882",
            "208c661c36eb4ca1bfbcb7d85e2ed4bc",
            "2a0336c6f3154a3d876b1138c74176aa",
            "9e8121e130924cb4b6482dbff71ecf49",
            "3d6c2648e9a944b7808605bfd99eeaef",
            "456701a295ac409f8c7475791988a812",
            "da37f1bc5a0a47c29d4c54619e4e2d5f",
            "e3675cc7283d4a9d89645487ffa88604",
            "84e75717d0f14726ab4b1d30985071d6",
            "3ccceae4464f4f42b1dd162274fb7150",
            "ad300568dc41444d8be31d6c2f67b369",
            "625f33da407040049ceac1376c6e29a3",
            "43aa7b0befbf43c789ec0d04f53168bb",
            "e787ebfad3ae46408c76cf85c6bc5227",
            "d8b61a7629f946aa8bfe0168737a54dc",
            "ffd66a5a2c9340e4a51f836cc50507ae",
            "315fd9febbd94336990818f0e6f1a469",
            "e6bba21ce63a4dabb48820159574b814",
            "91662f4ff7ed412986b30c88a0a1b21a",
            "9505af654e2043ffa18959c27b688434",
            "ee3414a9f48c4d90a609a1121973cb6a",
            "e85e8bb193df4851aa6ac8c197959c33",
            "f4b56bdba7b7461bbd1257d419319335",
            "9de63d372c9d4f098dbc39dba6ee1d87",
            "d08f43cf5d6543289df90c5cb469dcc7",
            "ddb0a08f58d146c4a8613587e5c70a67",
            "eddc1a51214f47edb401467199c360a8",
            "3e95f3d7742b46cfb71741e90cbcacf2",
            "8fa5231718bf4ca888205c141da58611",
            "cdf4ea6c0ef94c2096a2c0e7156a3034",
            "c24373738cb3453990b7188028ace67c",
            "28c41c763396495b9df1a68e0b05463b",
            "01d009fc7a6146b7b13e6bc87fbaa6dd",
            "6e192f61c1f84ed69ed0e8a671c3b36b",
            "008b3023191a499ea228562d19878fe7",
            "3921fa17cc684590ba142cd3d26d2457",
            "f44f8ff32dd14c9db95941c47929b9af",
            "18c8638f0ef349ceb9c28ee4e3a510bc",
            "e05a91d393d540928c12328fc4fd1224",
            "c523d89b857849a98743def116c08d73",
            "8dfd1966340344b8a628d123ec47c93a",
            "d599f9a346f84292b617df9338266f3a",
            "7f8cf88520044f85896e33848491ef09",
            "012796d923fc4b7c94908e9b7e525287",
            "564d72ba3d414cebbb0f68f9d30919e1",
            "0b0cc246ae4946349fc1e786a310010b",
            "b322a281897045809bdb39015a6e0dde",
            "54e828788255423cbb545984ed04cd93",
            "fee0788ce522437eabf18410194ea2dd",
            "2c3421022f5d45d7860a53be790b4aa4",
            "879088dc7fc44353a8b763e70c07119c",
            "574c0fb312d04dc09279c81998af5aa9",
            "6e5491d29cd94624ab16c50dc6cee2aa",
            "9516f9c6319f40dba47dc8fcccb9e586",
            "227464c2824f4989a769523fd72dbce4",
            "fd0a23dc36aa475d901cdeee39c1ff51",
            "790d29e5d9a046e6b1bea2021cbc7a1c",
            "7777070845b84b6bb9e28ee634b0763a",
            "ec575213783348dfac2a4b0c6d093576",
            "2865f351706d4387b0467062ae0c711f",
            "c6d16e048aa64b38bc53f41a1926d37b",
            "cd0262cb1c3649cebf44bc77b92da7fe",
            "54dec1379911491f950e11b0ecedbfaf",
            "16686feb7e7b4451830a7ee5be9635d7",
            "130ae039a8f54f3297ee891fa0c0808d",
            "73467cf0a6be4e83b4d74910799c5f85",
            "5ea1cb682ad4445896b0b0e8487e2e77",
            "46e1f93c466e402c8b2a5b2f76c6cff4",
            "b201c04f3be942018c34bf52e04cca98",
            "2da76e475806422d88f863b537d2d16c",
            "6d80ab2319fb4a17b7ccb4860e2ccc27",
            "1a91dcdd355c4ae8b62af5142d119321",
            "1718665a319f46859c7a1ec078b368f5"
          ]
        },
        "id": "pgSIhmIj0PjU",
        "outputId": "c554232a-4c82-462c-9779-e9e8729282c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "102c3e8ad45545d8b612e7709bb79428"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3675cc7283d4a9d89645487ffa88604"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91662f4ff7ed412986b30c88a0a1b21a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdf4ea6c0ef94c2096a2c0e7156a3034"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/958 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dfd1966340344b8a628d123ec47c93a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/812 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "574c0fb312d04dc09279c81998af5aa9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54dec1379911491f950e11b0ecedbfaf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_path = os.path.join(wokdir, CONFIG['data_subfolder'], f\"dev/{LANG}.csv\")\n",
        "dev_df = pd.read_csv(dev_path)\n",
        "\n",
        "def predict_dev_and_save(dev_df, trainer, tokenizer, wokdir, LANG, CONFIG):\n",
        "    # Add dummy labels\n",
        "    dev_dataset = PolarizationDataset(\n",
        "        dev_df['text'].tolist(),\n",
        "        [0] * len(dev_df),\n",
        "        tokenizer,\n",
        "        CONFIG['max_length']\n",
        "    )\n",
        "\n",
        "    # Predict\n",
        "    dev_preds = trainer.predict(dev_dataset)\n",
        "    predictions = np.argmax(dev_preds.predictions, axis=1)\n",
        "\n",
        "    # Save CSV\n",
        "    output_path = os.path.join(wokdir, f\"pred_{LANG}.csv\")\n",
        "    pd.DataFrame({\"id\": dev_df[\"id\"], \"polarization\": predictions}).to_csv(output_path, index=False)\n",
        "\n",
        "    return output_path, predictions\n",
        "\n",
        "output_path, predictions = predict_dev_and_save(dev_df, trainer, tokenizer, wokdir, LANG, CONFIG)\n"
      ],
      "metadata": {
        "id": "xayb_iAe1HSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREDICTION"
      ],
      "metadata": {
        "id": "uKPOdfW5t7Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "val_preds = trainer.predict(val_dataset)\n",
        "val_pred_labels = np.argmax(val_preds.predictions, axis=1)\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(val_df['polarization'], val_pred_labels,\n",
        "                          target_names=['Not Polarized', 'Polarized']))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CREATING PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dev_path = os.path.join(wokdir, CONFIG['data_subfolder'], f\"dev/{LANG}.csv\")\n",
        "dev_df = pd.read_csv(dev_path)\n",
        "print(f\"Dev samples: {len(dev_df)}\")\n",
        "\n",
        "dev_dataset = PolarizationDataset(\n",
        "    dev_df['text'].tolist(),\n",
        "    [0] * len(dev_df),\n",
        "    tokenizer,\n",
        "    CONFIG['max_length']\n",
        ")\n",
        "\n",
        "dev_preds = trainer.predict(dev_dataset)\n",
        "predictions = np.argmax(dev_preds.predictions, axis=1)\n",
        "\n",
        "output_path = os.path.join(wokdir, f\"noweight_pred_{LANG}.csv\")\n",
        "result_df = pd.DataFrame({\n",
        "    \"id\": dev_df[\"id\"],\n",
        "    \"polarization\": predictions\n",
        "})\n",
        "result_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Saved: {output_path}\")\n",
        "print(f\"Total predictions: {len(predictions)}\")\n",
        "print(f\"Distribution:\\n{pd.Series(predictions).value_counts()}\")\n",
        "print(f\"Distribution %:\\n{pd.Series(predictions).value_counts(normalize=True)*100}\")\n",
        "print(f\"Final F1 Binary: {final_eval.get('eval_f1_binary', 'N/A'):.4f}\")\n",
        "print(f\"Final F1 Macro: {final_eval.get('eval_f1_macro', 'N/A'):.4f}\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "id": "5xBn8vJt8XG7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "ccafa1dd-de40-4384-c299-14e4c3b09957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Detailed Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not Polarized       0.88      1.00      0.93        21\n",
            "    Polarized       1.00      0.75      0.86        12\n",
            "\n",
            "     accuracy                           0.91        33\n",
            "    macro avg       0.94      0.88      0.90        33\n",
            " weighted avg       0.92      0.91      0.91        33\n",
            "\n",
            "\n",
            "============================================================\n",
            "CREATING PREDICTIONS\n",
            "============================================================\n",
            "Dev samples: 160\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "COMPLETE\n",
            "============================================================\n",
            "Saved: /content/drive/MyDrive/BLOC#4/NLP/SemEval/noweight_pred_eng.csv\n",
            "Total predictions: 160\n",
            "Distribution:\n",
            "0    105\n",
            "1     55\n",
            "Name: count, dtype: int64\n",
            "Distribution %:\n",
            "0    65.625\n",
            "1    34.375\n",
            "Name: proportion, dtype: float64\n",
            "Final F1 Binary: 0.8571\n",
            "Final F1 Macro: 0.8952\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HAUSA :"
      ],
      "metadata": {
        "id": "XFc0XHxP2faE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== CONFIG ==================\n",
        "wokdir = \"/content/drive/MyDrive/BLOC#4/NLP/SemEval\"\n",
        "\n",
        "CONFIG = {\n",
        "    'language': 'swa',\n",
        "    'data_subfolder': \"dev_phase/subtask1/\",\n",
        "    'model_name': 'xlm-roberta-base',\n",
        "\n",
        "    'max_length': 300,\n",
        "    'num_epochs': 3,\n",
        "    'learning_rate': 3e-5,\n",
        "    'batch_size': 16,\n",
        "    'weight_decay': 0.01,\n",
        "    'warmup_ratio': 0.2,\n",
        "    'gradient_accumulation_steps': 4,\n",
        "    'val_split': 0.01,\n",
        "    'use_class_weights': True,\n",
        "    'use_kfold': True,\n",
        "    'n_folds': 5,\n",
        "}\n",
        "\n",
        "LANG = CONFIG['language']\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"IMPROVED TRAINING FOR: {LANG.upper()}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7uAGntWrhn5",
        "outputId": "8b8c040f-abe9-463a-fb2a-7c04f4efc623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "IMPROVED TRAINING FOR: SWA\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET LOADER"
      ],
      "metadata": {
        "id": "I-xlSFITtsTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PolarizationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer(\n",
        "            str(self.texts[idx]),\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=False,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item['labels'] = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
        "        return item\n"
      ],
      "metadata": {
        "id": "kff6RILorjco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUSTOM CLASS WEIGHTS WITH FOCAL LOSS"
      ],
      "metadata": {
        "id": "PSBEOZaYtl1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, class_weights=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if self.class_weights is not None:\n",
        "            loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "            loss = loss_fct(logits, labels)\n",
        "        else:\n",
        "            loss = outputs.loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# ================== METRICS ==================\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    labels = p.label_ids\n",
        "    return {\n",
        "        'f1_macro': f1_score(labels, preds, average='macro'),\n",
        "        'f1_binary': f1_score(labels, preds, pos_label=1),\n",
        "        'f1_weighted': f1_score(labels, preds, average='weighted'),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "zRulQXVkrmVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== LOAD DATA ==================\n",
        "print(f\"\\nLoading training data for {LANG}...\")\n",
        "train_path = os.path.join(wokdir, CONFIG['data_subfolder'], f\"train/{LANG}.csv\")\n",
        "df = pd.read_csv(train_path)\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"Class distribution:\\n{df['polarization'].value_counts()}\")\n",
        "print(f\"Class distribution %:\\n{df['polarization'].value_counts(normalize=True)*100}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNigaypTruz8",
        "outputId": "1ae3d7e6-4de2-4810-c6e7-66f70467ec6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading training data for swa...\n",
            "Total samples: 6991\n",
            "Class distribution:\n",
            "polarization\n",
            "1    3504\n",
            "0    3487\n",
            "Name: count, dtype: int64\n",
            "Class distribution %:\n",
            "polarization\n",
            "1    50.121585\n",
            "0    49.878415\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA IMBALANCE TREATEMENT"
      ],
      "metadata": {
        "id": "5ModmUectfh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for class imbalance\n",
        "class_counts = df['polarization'].value_counts()\n",
        "imbalance_ratio = class_counts.max() / class_counts.min()\n",
        "print(f\"\\n  Class imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "if imbalance_ratio > 2:\n",
        "    print(\" HIGH IMBALANCE DETECTED - Using class weights!\")\n",
        "    CONFIG['use_class_weights'] = True\n",
        "\n",
        "# ================== COMPUTE CLASS WEIGHTS ==================\n",
        "class_weights = None\n",
        "if CONFIG['use_class_weights']:\n",
        "    classes = df['polarization'].unique()\n",
        "    weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=classes,\n",
        "        y=df['polarization']\n",
        "    )\n",
        "    class_weights = torch.FloatTensor(weights).cuda() if torch.cuda.is_available() else torch.FloatTensor(weights)\n",
        "    print(f\"\\nClass weights: {weights}\")\n",
        "\n",
        "# Split data\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=CONFIG['val_split'],\n",
        "    stratify=df['polarization'],\n",
        "    random_state=42\n",
        ")\n",
        "print(f\"\\nTrain: {len(train_df)} | Val: {len(val_df)}\")\n",
        "\n",
        "# ================== TOKENIZER & MODEL ==================\n",
        "print(f\"\\nLoading tokenizer and model: {CONFIG['model_name']}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    CONFIG['model_name'],\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# ================== CREATE DATASETS ==================\n",
        "train_dataset = PolarizationDataset(\n",
        "    train_df['text'].tolist(),\n",
        "    train_df['polarization'].tolist(),\n",
        "    tokenizer,\n",
        "    CONFIG['max_length']\n",
        ")\n",
        "val_dataset = PolarizationDataset(\n",
        "    val_df['text'].tolist(),\n",
        "    val_df['polarization'].tolist(),\n",
        "    tokenizer,\n",
        "    CONFIG['max_length']\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTdtedqYr6Sl",
        "outputId": "8bba8c4f-d856-402e-d4a4-f89d83d9d650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Class imbalance ratio: 1.00:1\n",
            "\n",
            "Class weights: [0.9975742  1.00243763]\n",
            "\n",
            "Train: 6921 | Val: 70\n",
            "\n",
            "Loading tokenizer and model: xlm-roberta-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING PARAMETERS"
      ],
      "metadata": {
        "id": "5g3YjBt3tbBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ================== TRAINING ARGUMENTS ==================\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./results_{LANG}\",\n",
        "    num_train_epochs=CONFIG['num_epochs'],\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    per_device_train_batch_size=CONFIG['batch_size'],\n",
        "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
        "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
        "    weight_decay=CONFIG['weight_decay'],\n",
        "    warmup_ratio=CONFIG['warmup_ratio'],\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,  # More frequent evaluation\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    logging_steps=25,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# ================== TRAINER ==================\n",
        "trainer_class = WeightedTrainer if CONFIG['use_class_weights'] else Trainer\n",
        "\n",
        "trainer = trainer_class(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
        "    class_weights=class_weights if CONFIG['use_class_weights'] else None\n",
        ")\n"
      ],
      "metadata": {
        "id": "ahIiNZZhr9aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN"
      ],
      "metadata": {
        "id": "ahgte-ZmtWKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL VALIDATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "final_eval = trainer.evaluate()\n",
        "print(final_eval)\n",
        "\n",
        "# Detailed classification report on validation set\n",
        "val_preds = trainer.predict(val_dataset)\n",
        "val_pred_labels = np.argmax(val_preds.predictions, axis=1)\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(val_df['polarization'], val_pred_labels,\n",
        "                          target_names=['Not Polarized', 'Polarized']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "1uBgDgfvr_8j",
        "outputId": "70820b93-02ae-4b30-dff2-87d36a46942a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STARTING TRAINING\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='327' max='327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [327/327 09:58, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Binary</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.668200</td>\n",
              "      <td>0.581441</td>\n",
              "      <td>0.714052</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.714052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.611900</td>\n",
              "      <td>0.462446</td>\n",
              "      <td>0.812410</td>\n",
              "      <td>0.793651</td>\n",
              "      <td>0.812410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.493100</td>\n",
              "      <td>0.472217</td>\n",
              "      <td>0.812410</td>\n",
              "      <td>0.793651</td>\n",
              "      <td>0.812410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.506200</td>\n",
              "      <td>0.454682</td>\n",
              "      <td>0.785671</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.785671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.465000</td>\n",
              "      <td>0.403277</td>\n",
              "      <td>0.813333</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.813333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.482600</td>\n",
              "      <td>0.390797</td>\n",
              "      <td>0.813333</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.813333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL VALIDATION RESULTS\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4032771587371826, 'eval_f1_macro': 0.8133333333333334, 'eval_f1_binary': 0.8, 'eval_f1_weighted': 0.8133333333333334, 'eval_runtime': 0.1235, 'eval_samples_per_second': 566.714, 'eval_steps_per_second': 40.48, 'epoch': 3.0}\n",
            "\n",
            "Detailed Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not Polarized       0.78      0.89      0.83        35\n",
            "    Polarized       0.87      0.74      0.80        35\n",
            "\n",
            "     accuracy                           0.81        70\n",
            "    macro avg       0.82      0.81      0.81        70\n",
            " weighted avg       0.82      0.81      0.81        70\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVING MODEL TO HUGGING FACE"
      ],
      "metadata": {
        "id": "nW2B4Fce7xso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, Repository\n",
        "from huggingface_hub import upload_folder\n",
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "2bc0a0be58634091a8cf741088ede7c1",
            "494bec21bb304e5cb7a2ad7f7fcb1436",
            "ae247034497446138d547b435f103946",
            "b4d4be13e3a74c09ab8bb20b6d08c6cb",
            "ddc1725abd4d42ca8f92c9531b20c5d1",
            "1275a9da885f4bd99336414875298657",
            "0c5f222b637c4325b755e7920a5d346e",
            "8a515959cc244911844b0260001defe3",
            "cfa568c78a224ae59bfd1ab55e719c48",
            "baa91e96a1e94f298e0660d9c844eaca",
            "cac66cc6081649a7aaa7c9c00864fde7",
            "d541963069544e11a4a072ca6c3423e7",
            "5a3feb5f77d64896865b5db7f7e2ad34",
            "46f94c9ec722425cafd1af4ba4e2be7d",
            "74b6677c38834dd38d42015ef2033ad6",
            "2c045e4645ce4c0e8eea69ed747a352b",
            "087126522ce04114a8b0990fc6868ca4",
            "ae30b93711b34348b6290da37d7f096f",
            "8a13a3d1255a4f04b43fda88e8c43ea4",
            "aa456bd8e2c04b4cbf3aeb89dc488bb6"
          ]
        },
        "outputId": "e10d3c81-8562-40d3-d009-e58359debd14",
        "id": "THfQIC_N7xsp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bc0a0be58634091a8cf741088ede7c1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_folder = os.path.join(wokdir, 'SUBTASK_1_HAU')\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(model_folder)\n",
        "tokenizer.save_pretrained(model_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee81087-c661-41eb-9338-67f04453a4f3",
        "id": "f3zsldea7xss"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/BLOC#4/NLP/SemEval/SUBTASK_1_HAU/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/BLOC#4/NLP/SemEval/SUBTASK_1_HAU/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/BLOC#4/NLP/SemEval/SUBTASK_1_HAU/sentencepiece.bpe.model',\n",
              " '/content/drive/MyDrive/BLOC#4/NLP/SemEval/SUBTASK_1_HAU/added_tokens.json',\n",
              " '/content/drive/MyDrive/BLOC#4/NLP/SemEval/SUBTASK_1_HAU/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HF_MODEL_NAME = \"mirindraf/aims-sentiment-analysis-hau\"\n",
        "api = HfApi()\n",
        "api.create_repo(\n",
        "    repo_id=HF_MODEL_NAME,\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True\n",
        ")\n",
        "upload_folder(\n",
        "    folder_path=model_folder,\n",
        "    repo_id=HF_MODEL_NAME,\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "dcec4880033241579fdcc4117bbeccb4",
            "a61373f952ce4b7998622f655045e133",
            "52f472c45c3a4bef8e6fb79a730e6923",
            "a5befe29275149a4af6ae0880ee0bba4",
            "582b0c1edf9c41429600bccd892ed157",
            "f89b07dd50a0461d904a570fff8d0a65",
            "2ae70171404a4c3e9438f5b61fef898b",
            "b9ebaa3f2f53419d9d754e247e02dfbc",
            "f4f93a7c472641b5807d7e9ade131ba9",
            "cecc18b82ea14bb19445e7ca4fd91547",
            "f874572339d746fc9460f9f8af667184",
            "444ed5ae44454830b641a16febca1053",
            "60474db733644c22951f1776b027da22",
            "c5212ae04d274414aa9591845dce0bab",
            "decee741f17b414c98663de7878cbe31",
            "fb47e0644c9a4295802997572e2f776a",
            "940adc6e74e24f77af46bab02b07e0cb",
            "4e58f77b33c74f8dbb76a388c0252f54",
            "1a8d2177c89f474e83bb46610b0948a4",
            "264f83867c7f45f6ab33d3dfc3ffe985",
            "a1e2b28f909e48809ef4bea9f5660e58",
            "06780149e827424d9b4fab5688c4099e",
            "14b69f73c53b4b858a30ee83203f995c",
            "8089bdd1ef52482fa70b194932717db4",
            "d0ed079663084725a7dd8c57f435c8a4",
            "d6e3565d179b4a98817124b7f2929eba",
            "6cf608b229e94047b02dc8c70dda5232",
            "b6eb28e966344b75abe218d842a1c89e",
            "31875a1627fa408a9fa98b2d8341eae6",
            "dc1ce446270d4567924fb44ff83aab52",
            "a399b09f434a485d8b594f4d2a64c740",
            "0e13008a3e7b466892c4962e2f3928fd",
            "4f51e380a70a451c9f978cd7000a01ab",
            "59c7d5d2f663459da81abadb5ed4b2e9",
            "e5c26bd3622441f3b0ee88b1a8ca7a36",
            "8c6f470228ff433bad7ba9b1c5682931",
            "0e32ac73013e4d9695cc60f76aefe913",
            "5f98603ed7924ac9b73493c86842c216",
            "54f772813e49472596632870521ddb53",
            "ee64c814f2ce4bfe8f1c00d7f7e60dd5",
            "c401dbb439a44567b77cfe44731bfb48",
            "e6d40864e5cf435ebf0ef3b3e91f1a0d",
            "2eb6fb79aa2f4eb18a8d3b7f5ac6c8e3",
            "3d75e69e0b3a43c4a520a5e70ea609cc",
            "4d02f4c2872340789214ed67299826be",
            "e88a53baa87349d489c5d8224b66509f",
            "c380018a17e84f4eb1bb222ab434cf71",
            "53bef2416e4b4a77b2b67cd2c4266b13",
            "00c723938da34144812326ff4f856b0f",
            "75a3597936ea4edb95c1895a41fdd157",
            "eab934c9dd49435fb853e366553845c7",
            "afb7e846f40844dd8708d26ac203d80d",
            "5c4cd1d43c62498d93d463eb48018cb8",
            "cd1ac5dd628b44d4b8b01364f1223058",
            "622db85a3274415fbdcde451e31d4752"
          ]
        },
        "outputId": "16c76870-8284-483f-d71c-5b9cf5706e41",
        "id": "eyUQppML7xsv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcec4880033241579fdcc4117bbeccb4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "444ed5ae44454830b641a16febca1053"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...TASK_1_HAU/tokenizer.json:  97%|#########6| 16.6MB / 17.1MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14b69f73c53b4b858a30ee83203f995c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...U/sentencepiece.bpe.model: 100%|##########| 5.07MB / 5.07MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59c7d5d2f663459da81abadb5ed4b2e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...K_1_HAU/model.safetensors:   0%|          |  560kB / 1.11GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d02f4c2872340789214ed67299826be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/mirindraf/aims-sentiment-analysis-hau/commit/45abefe5d383a4d6869b1b50ebf712ff70d8cd4d', commit_message='Upload folder using huggingface_hub', commit_description='', oid='45abefe5d383a4d6869b1b50ebf712ff70d8cd4d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mirindraf/aims-sentiment-analysis-hau', endpoint='https://huggingface.co', repo_type='model', repo_id='mirindraf/aims-sentiment-analysis-hau'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load from hugging face\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(HF_MODEL_NAME)\n",
        "# Create a new Trainer (no training needed, just for inference)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "406907f19e484db8a82543bfd15ee5b5",
            "6f3bb606537c44abb6a10a273bc042a5",
            "7a55944858814cd3ab46058ba2c5d453",
            "b602925907504974bf013da7e4ebb965",
            "1f4640845a5f46aca62da29778193f36",
            "8127df0510084db5b39de20f1642c6e7",
            "2b587b36afa145a081052a10addb8c06",
            "ca1c84373e7a47119a1080dde7d42d51",
            "07914fe3ca7045d09f02d41f87715e38",
            "317678113588435eb445d6de6f760c9c",
            "737ff9721d564894a6097b52b75ca747",
            "4c7dbfaadd5e4bf68fe784842faf2cf5",
            "8c7af72c51c6438eb04d1abac75b7902",
            "b02a3ed286b941d18574cd702378804d",
            "02f03c4e3865479f94b9ad834046a9ba",
            "05738734a0a34eddb9373d299edf63bc",
            "98c14bf9537f4021b98b8206babfe971",
            "22d26ac62487459ca7e1e2a20b4c3ea4",
            "54473fc9d9b646c59b1170a79b4476cd",
            "eb9f06ddffa646138f8bfef61d5bbe48",
            "056671ab918a4f4fa60e72414535bd52",
            "82b5fd2be6df4066a824f07ee014f641",
            "9cf86d1a8bb543f9a95c1b54b6799410",
            "e00e243b60e14631b21704a73fb6b3ea",
            "95d5c01aeeee4f7584a3a5737734d379",
            "2fc6000e75784ed1a90e69bcffc37e29",
            "c694a0e7584346dcaa05dd5670ba884b",
            "906a71f95f1649a4bd6b7245c813e805",
            "d5929dad1d0a42b3a19fdf34d7166b9b",
            "d7bab08399664fbdb211ca4a2a339c1b",
            "67bb6e3ae1854f53afaa2849d238c9f7",
            "61fd0b2d48744fa8b2e2ed9c8c007fb2",
            "ae6c334195564939a106a02cfcdc81df",
            "a2d92cfa730a497abc47b2a3ad66349a",
            "6e98b57203d54ce4891c5ab136a1adcd",
            "ca9d7fa337764bcb91b21078a3889c50",
            "90f08d4a5a9b4428ab98d48b19c4c7c6",
            "7212df8fee0641de8db03eea90869427",
            "28f49f0c1c834463b9198e9efabebfc1",
            "11ef3517296043518d57e0e58aaf2b34",
            "6a7ce8e793e94c4fb152708376b6fc3e",
            "ad18c4ca691340179ed41d125876e9bd",
            "68b279c2afdf4d8ea0b25eb1e4454ac8",
            "99fcbd0bb4844a00a95ab716b003ba90",
            "463e39b594ab4667bed1de8d70569ffe",
            "6d596abc4b414e9091069ec48fd6aa7a",
            "4a37f9006579487c9cadd5e651f88469",
            "a355c96c93fe49d9ab3048147d2345b8",
            "5c3578ff06a54ce8b96418378f80934f",
            "6c8efff0ca4a4fb9b92387b35f5aa7d0",
            "494cce2e344d4572aca260fa285b798c",
            "b0ae8f79194044d387fafa2f614849ea",
            "a59a968065e24fe3908d10ae8358c566",
            "fbb430d9adc346cb9335b854abff96be",
            "8afb8b8e06064feea07ddc07bab8ef94",
            "b89670012a834636ad588426b164d982",
            "6325480dbeb54405a34dad2ad30ee06f",
            "f689010bbe17426aa69e8a589495507d",
            "5ceb4340d6884119ac5bcb01d0440f25",
            "8c586ea489c54514a6b27631730e134e",
            "5e34ef98fc5c45969f449e6bd91be2cc",
            "40b1f5b1a41d4112872a56d01f4f1757",
            "a97aa98d671c4f37af7c436f1a758164",
            "29d22e4d0b7647a2b56cf63c72677df2",
            "f57f0fdd49b849fd9df71fd013628d14",
            "2f010afeaffe43d9adb5055b94909bb0"
          ]
        },
        "outputId": "d57fab55-4ee3-46df-afa4-d16f522e38d4",
        "id": "9HJOohwS7xsx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "406907f19e484db8a82543bfd15ee5b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c7dbfaadd5e4bf68fe784842faf2cf5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cf86d1a8bb543f9a95c1b54b6799410"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2d92cfa730a497abc47b2a3ad66349a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/676 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "463e39b594ab4667bed1de8d70569ffe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b89670012a834636ad588426b164d982"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_path = os.path.join(wokdir, CONFIG['data_subfolder'], f\"dev/{LANG}.csv\")\n",
        "dev_df = pd.read_csv(dev_path)\n",
        "\n",
        "def predict_dev_and_save(dev_df, trainer, tokenizer, wokdir, LANG, CONFIG):\n",
        "    # Add dummy labels\n",
        "    dev_dataset = PolarizationDataset(\n",
        "        dev_df['text'].tolist(),\n",
        "        [0] * len(dev_df),\n",
        "        tokenizer,\n",
        "        CONFIG['max_length']\n",
        "    )\n",
        "\n",
        "    # Predict\n",
        "    dev_preds = trainer.predict(dev_dataset)\n",
        "    predictions = np.argmax(dev_preds.predictions, axis=1)\n",
        "\n",
        "    # Save CSV\n",
        "    output_path = os.path.join(wokdir, f\"pred_{LANG}.csv\")\n",
        "    pd.DataFrame({\"id\": dev_df[\"id\"], \"polarization\": predictions}).to_csv(output_path, index=False)\n",
        "\n",
        "    return output_path, predictions\n",
        "\n",
        "output_path, predictions = predict_dev_and_save(dev_df, trainer, tokenizer, wokdir, LANG, CONFIG)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "collapsed": true,
        "outputId": "a29a6403-a5ab-4314-9e42-1c04e9f7fedd",
        "id": "St6zZhCo7xsz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PREDICTION PIPELINE"
      ],
      "metadata": {
        "id": "DN-i3WXztUiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CREATING PREDICTION PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create pipeline from trained model\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=trainer.model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    max_length=CONFIG['max_length'],\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Load dev data\n",
        "print(f\"\\nLoading dev data for {LANG}...\")\n",
        "dev_path = os.path.join(wokdir, CONFIG['data_subfolder'], f\"dev/{LANG}.csv\")\n",
        "dev_df = pd.read_csv(dev_path)\n",
        "print(f\"Dev samples: {len(dev_df)}\")\n",
        "\n",
        "# Make predictions\n",
        "print(\"\\nGenerating predictions...\")\n",
        "predictions = []\n",
        "batch_size = 32\n",
        "\n",
        "for i in tqdm(range(0, len(dev_df), batch_size)):\n",
        "    batch_texts = dev_df['text'].iloc[i:i+batch_size].tolist()\n",
        "    batch_preds = classifier(batch_texts)\n",
        "\n",
        "    # Extract labels (LABEL_0 -> 0, LABEL_1 -> 1)\n",
        "    batch_labels = [int(pred['label'].split('_')[1]) for pred in batch_preds]\n",
        "    predictions.extend(batch_labels)\n",
        "\n",
        "# ================== SAVE PREDICTIONS ==================\n",
        "output_path = os.path.join(wokdir, f\"pred_{LANG}.csv\")\n",
        "result_df = pd.DataFrame({\n",
        "    \"id\": dev_df[\"id\"],\n",
        "    \"polarization\": predictions\n",
        "})\n",
        "result_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTION COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Saved predictions to: {output_path}\")\n",
        "print(f\"Total predictions: {len(predictions)}\")\n",
        "print(f\"Prediction distribution:\\n{pd.Series(predictions).value_counts()}\")\n",
        "print(f\"Prediction distribution %:\\n{pd.Series(predictions).value_counts(normalize=True)*100}\")\n",
        "print(\"\\n Done!\")\n"
      ],
      "metadata": {
        "id": "qXMmaaybsDar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0decac8b-733b-4f9b-a68c-796bf33709e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CREATING PREDICTION PIPELINE\n",
            "============================================================\n",
            "\n",
            "Loading dev data for swa...\n",
            "Dev samples: 349\n",
            "\n",
            "Generating predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 11/11 [00:04<00:00,  2.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PREDICTION COMPLETE!\n",
            "============================================================\n",
            "Saved predictions to: /content/drive/MyDrive/BLOC#4/NLP/SemEval/pred_swa.csv\n",
            "Total predictions: 349\n",
            "Prediction distribution:\n",
            "0    189\n",
            "1    160\n",
            "Name: count, dtype: int64\n",
            "Prediction distribution %:\n",
            "0    54.154728\n",
            "1    45.845272\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            " Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SUMMARY"
      ],
      "metadata": {
        "id": "RN5lLPPCtMRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Language: {LANG}\")\n",
        "print(f\"Model: {CONFIG['model_name']}\")\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(val_df)}\")\n",
        "print(f\"Class weights used: {CONFIG['use_class_weights']}\")\n",
        "print(f\"Final F1 Binary: {final_eval.get('eval_f1_binary', 'N/A'):.4f}\")\n",
        "print(f\"Final F1 Macro: {final_eval.get('eval_f1_macro', 'N/A'):.4f}\")\n",
        "print(\"=\"*60)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CcyWxors2oE5",
        "outputId": "3308178c-c022-46c4-f12e-f807bb5122a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "IMPROVED TRAINING FOR: HAU\n",
            "============================================================\n",
            "\n",
            "Loading training data for hau...\n",
            "Total samples: 3651\n",
            "Class distribution:\n",
            "polarization\n",
            "0    3259\n",
            "1     392\n",
            "Name: count, dtype: int64\n",
            "Class distribution %:\n",
            "polarization\n",
            "0    89.263216\n",
            "1    10.736784\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "  Class imbalance ratio: 8.31:1\n",
            "     HIGH IMBALANCE DETECTED - Using class weights!\n",
            "\n",
            "Class weights: [0.56014115 4.65688776]\n",
            "\n",
            "Train: 3614 | Val: 37\n",
            "\n",
            "Loading tokenizer and model: xlm-roberta-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STARTING TRAINING\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [570/570 12:59, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Binary</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.645300</td>\n",
              "      <td>0.450290</td>\n",
              "      <td>0.766709</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.885314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.533800</td>\n",
              "      <td>0.272980</td>\n",
              "      <td>0.839827</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.928045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.511400</td>\n",
              "      <td>0.247093</td>\n",
              "      <td>0.839827</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.928045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.413100</td>\n",
              "      <td>0.065638</td>\n",
              "      <td>0.936752</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.974267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.428800</td>\n",
              "      <td>0.142963</td>\n",
              "      <td>0.884375</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.950507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.390200</td>\n",
              "      <td>0.081875</td>\n",
              "      <td>0.936752</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.974267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.276800</td>\n",
              "      <td>0.070934</td>\n",
              "      <td>0.936752</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.974267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.234700</td>\n",
              "      <td>0.047249</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.233600</td>\n",
              "      <td>0.044146</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.267900</td>\n",
              "      <td>0.042196</td>\n",
              "      <td>0.936752</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.974267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.176400</td>\n",
              "      <td>0.035343</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL VALIDATION RESULTS\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.047249045222997665, 'eval_f1_macro': 1.0, 'eval_f1_binary': 1.0, 'eval_f1_weighted': 1.0, 'eval_runtime': 0.1345, 'eval_samples_per_second': 275.073, 'eval_steps_per_second': 22.303, 'epoch': 10.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Detailed Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not Polarized       1.00      1.00      1.00        33\n",
            "    Polarized       1.00      1.00      1.00         4\n",
            "\n",
            "     accuracy                           1.00        37\n",
            "    macro avg       1.00      1.00      1.00        37\n",
            " weighted avg       1.00      1.00      1.00        37\n",
            "\n",
            "\n",
            "============================================================\n",
            "CREATING PREDICTION PIPELINE\n",
            "============================================================\n",
            "\n",
            "Loading dev data for hau...\n",
            "Dev samples: 182\n",
            "\n",
            "Generating predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 6/6 [00:02<00:00,  2.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PREDICTION COMPLETE!\n",
            "============================================================\n",
            "Saved predictions to: /content/drive/MyDrive/BLOC#4/NLP/SemEval/pred_hau.csv\n",
            "Total predictions: 182\n",
            "Prediction distribution:\n",
            "0    157\n",
            "1     25\n",
            "Name: count, dtype: int64\n",
            "Prediction distribution %:\n",
            "0    86.263736\n",
            "1    13.736264\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            " Done!\n",
            "\n",
            "============================================================\n",
            "TRAINING SUMMARY\n",
            "============================================================\n",
            "Language: hau\n",
            "Model: xlm-roberta-base\n",
            "Training samples: 3614\n",
            "Validation samples: 37\n",
            "Class weights used: True\n",
            "Final F1 Binary: 1.0000\n",
            "Final F1 Macro: 1.0000\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
